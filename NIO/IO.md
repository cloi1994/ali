### 什么是socket？什么是I/O操作?

unix(like)世界里，一切皆文件 (0101 stream) : **socket,FIFO、管道、终端**

**socket 是实现tcp一套接口，有create、listen、connect、accept、send、read和write等基础的函数提供，方便我们更好的来实现tcp网络传输，操作就会转化为对这个描述符的操作(fd)** 

<不同的进程中会出现相同的文件描述符，它们可能指向同一个文件，也可能指向不同的文件>

<两个不同的文件描述符，若指向同一个打开文件句柄，将共享同一文件偏移量。因此，如果通过其中一个文件描述符来修改文件偏移量（由调用read()、write()或lseek()所致），那么从另一个描述符中也会观察到变化，无论这两个文件描述符是否属于不同进程，还是同一个进程，情况都是如此>

### 同步异步，阻塞非阻塞区别联系

**同步**

阻塞: 触发IO操作并等待 



非阻塞: 进程给CPU传达任务后，继续处理后续的操作，隔断时间再来询问之前的操作是否完成（不停的去检测数据）＝》（cpu的空转）

```c
while true  
{  
    for i in stream[]  
    {  
        if i has data  
        read until unavailable  
    }  
}  
```

多路IO:

不让这个线程亲自去检查流中是否有事件，而是引进了一个代理(一开始是select,后来是poll)，这个代理很牛，它可以同时观察许多流的I/O事件，如果没有事件，代理就阻塞，线程就不会挨个挨个去轮询

**select具有O(n)的无差别轮询复杂度**，同时处理的流越多，无差别轮询时间就越长。（2048 fd数量限制)

**poll**

它没有最大连接数的限制，原因是它是基于链表来存储的，但是同样有一个缺点：

1、大量的fd的数组被整体复制于用户态和内核地址空间之间，而不管这样的复制是不是有意义。                   

2、poll还有一个特点是“水平触发”，如果报告了fd后，没有被处理，那么下次poll时会再次报告该fd。



```c
while true  
{  
    select(streams[]) //这一步死在这里，知道有一个流有I/O事件时，才往下执行  
    for i in streams[]  
    {  
        if i has data  
        read until unavailable  
    }  
} 
```



**epoll可以理解为event poll**: epoll会把哪个流发生了怎样的I/O事件通知我们。所以我们说epoll实际上是**事件驱动（每个事件关联上fd）**的，此时我们对这些流的操作都是有意义的。**复杂度降低到了O(1)，可以多些FD**

1. 表面上看epoll的性能最好，但是在连接数少并且连接都十分活跃的情况下，select和poll的性能可能比epoll好，毕竟epoll的通知机制需要很多函数回调。

2. select低效是因为每次它都需要轮询。但低效也是相对的，视情况而定，也可通过良好的设计改善

```c
while true  
{  
    active_stream[] = epoll_wait(epollfd)  
    for i in active_stream[]  
    {  
        read or write till  
    }  
}  
```

 

Trade Off

**水平触发**:如果文件描述符已经就绪可以非阻塞的执行IO操作了,此时会触发通知.允许在任意时刻重复检测IO的状态.select,poll就属于水平触发.如果报告了fd后，没有被处理，那么下次poll时会再次报告该fd。

优点：当进行socket通信的时候，保证了数据的完整输出，进行IO操作的时候，如果还有数据，就会一直的通知你。

缺点：由于只要还有数据，内核就会不停的从内核空间转到用户空间，所有占用了大量内核资源，试想一下当有大量数据到来的时候，每次读取一个字节，这样就会不停的进行切换。内核资源的浪费严重。效率来讲也是很低的。

 **边缘触发**: 如果文件描述符自上次状态改变后有新的IO活动到来,它只会提示一次，直到下次再有数据流入之前都不会再提示了 ＝》清空Ready Status'

无论fd中是否还有数据可读。所以在ET模式下，read一个fd的时候一定要把它的buffer读光，也就是说一直读到read的返回值小于请求值，或者 遇到EAGAIN错误。 ＝》会因为可读这个状态没有发生变化而陷入长期等待

优点：每次内核只会通知一次，大大减少了epoll事件被重复触发的次数和内核资源的浪费，提高效率。

缺点：不能保证数据的完整。不能及时的取出所有的数据。



##### 使用非阻塞的文件描述符

对于读：由于需要一直读直到把数据读完，所以大家在编写程序的时候一般会用一个循环一直读取socket，那这个循环势必会在最后一次阻塞，即没有数据可读的情况下，阻塞式socket会在数据读完之后一直阻塞下去，而非阻塞式的socket则返回<0，并让errno 返回 EAGAIN

**饥饿**

ET本身并不会造成饥饿，由于事件只通知一次，开发者一不小心就容易遗漏了待处理的数据，像是饥饿，实质是bug

如果I/O数据量很大，可能在读取数据的过程中其他文件得不到处理，造成饥饿。解决方法是维护一个就绪列表，和很的调度(Scheduling)，在关联数据结构中标记文件描述符为就绪状态，由此可以记住哪些文件在等待，并对所有就绪文件作轮转处理

##### epoll应用场景

适合用epoll的应用场景：对于连接特别多，活跃的连接特别少，这种情况等的时间特别久，典型的应用场景为一个需要处理上万的连接服务器，例如各种app的入口服务器，例如qq

不适合epoll的场景：连接比较少，数据量比较大，例如ssh

epoll 的惊群问题：因为epoll 多用于 多个连接，只有少数活跃的场景，但是万一某一时刻，epoll 等的上千个文件描述符都就绪了，这时候epoll 要进行大量的I/O,此时压力太大。



**异步**

进程触发IO操作以后，直接返回，做自己的事情，IO交给内核来处理，完成后内核通知进程IO完成。

真正的异步IO需要CPU的深度参与。只有用户线程在操作IO的时候根本不去考虑IO的执行全部都交给CPU去完成，而自己只等待一个完成信号的时候，才是真正的异步IO。所以，拉一个子线程去轮询、去死循环，或者使用select、poll、epool，都不是异步。

#### IO流程：

1. 等待数据准备好（waiting for data to be ready）。对于一个套接口上的操作，这一步骤关系到数据从网络到达，并将其复制到内核的某个缓冲区。
2. 将数据从内核缓冲区复制到进程缓冲区**（**copying the data from the kernel to the process）。

##### Reactor设计模式 I/O复用模型（加強）

![image-20190130164508743](/Users/ck/Library/Application Support/typora-user-images/image-20190130164508743.png)通过Reactor的方式，可以将用户线程轮询IO操作状态的工作统一交给handle_events事件循环进行处理。用户线程注册事件处理器之后可以继续执行做其他的工作（异步）

而Reactor线程负责调用内核的select函数检查socket状态。当有socket被激活时，则通知相应的用户线程（或执行用户线程的回调函数），执行**handle_event**进行数据读取、处理的工作。由于select函数(Reactor)是阻塞的，因此多路IO复用模型也被称为异步阻塞IO模型 **socket都是设置为NONBLOCK的**



#### AIO

![image-20190130160840117](/Users/ck/Library/Application Support/typora-user-images/image-20190130162050309.png)

1. 用户线程直接使用内核提供的异步IO API发起read请求，且发起后立即返回，继续执行用户线程代码。
2. 不过此时用户线程已经将调用的**AsynchronousOperation和CompletionHandler注册到内核**，然后操作系统开启独立的内核线程去处理IO操作。
3. 当read请求的数据到达时，由内核负责读取socket中的数据，并写入用户指定的缓冲区中。最后内核将read的数据和用户线程注册的CompletionHandler分发给内部Proactor，Proactor将IO完成的信息通知给用户线程（一般通过调用用户线程注册的完成事件处理函数），完成异步IO。